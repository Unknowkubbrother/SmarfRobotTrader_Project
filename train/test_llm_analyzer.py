# ==========================================
# ‡πÑ‡∏ü‡∏•‡πå: test_llm_analyzer.py
# ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö LLM Analyzer ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á
# ==========================================

import numpy as np
from llm_analyzer import LLMTrainingAnalyzer

def test_llm_analyzer():
    """
    ‡∏ó‡∏î‡∏™‡∏≠‡∏ö LLM Analyzer ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á
    """
    print("="*80)
    print("üß™ Testing LLM Analyzer")
    print("="*80)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á analyzer
    analyzer = LLMTrainingAnalyzer()
    
    if not analyzer.enabled:
        print("\n‚ö†Ô∏è LLM Analyzer is disabled.")
        print("Please set GOOGLE_API_KEY environment variable:")
        print("  Windows (PowerShell): $env:GOOGLE_API_KEY='your-key'")
        print("  Windows (CMD): set GOOGLE_API_KEY=your-key")
        print("  Linux/Mac: export GOOGLE_API_KEY='your-key'")
        return
    
    print("\nüìä Generating simulated training metrics...")
    
    # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô 25 iterations
    # Scenario: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡∏î‡∏µ
    for i in range(1, 26):
        # Policy loss ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ
        policy_loss = -0.02 - (i * 0.001) + np.random.normal(0, 0.005)
        
        # Value loss ‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ ‡∏•‡∏î‡∏•‡∏á
        value_loss = 0.05 - (i * 0.001) + np.random.normal(0, 0.003)
        
        # Entropy ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ (policy converging)
        entropy_loss = -1.1 + (i * 0.02) + np.random.normal(0, 0.03)
        
        # Explained variance ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ (‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ 1.0)
        explained_variance = 0.2 + (i * 0.03) + np.random.normal(0, 0.05)
        explained_variance = min(explained_variance, 1.0)  # ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 1.0
        
        metrics = {
            'loss': policy_loss,
            'value_loss': value_loss,
            'entropy_loss': entropy_loss,
            'explained_variance': explained_variance,
            'approx_kl': 0.01 + np.random.normal(0, 0.002),
            'clip_fraction': 0.1 + np.random.normal(0, 0.02)
        }
        
        analyzer.log_metrics(i, metrics)
        
        if i % 5 == 0:
            print(f"  ‚úì Logged metrics for iteration {i}/25")
    
    print("\nüìà Creating training chart...")
    chart_path = analyzer.create_training_chart('test_training_chart.png')
    
    if not chart_path:
        print("‚ùå Failed to create chart")
        return
    
    print(f"‚úÖ Chart created: {chart_path}")
    
    print("\nü§ñ Analyzing with LLM...")
    print("   (This may take a few seconds...)")
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ LLM
    analysis = analyzer.analyze_with_llm(
        chart_path,
        current_iteration=25,
        total_iterations=50
    )
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
    analyzer.print_analysis(analysis)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•
    analyzer.save_analysis(analysis, 'test_llm_analysis.json')
    
    print("\n" + "="*80)
    print("‚úÖ Test completed successfully!")
    print("="*80)
    print("\nGenerated files:")
    print("  üìä test_training_chart.png - Training progress chart")
    print("  üìÑ test_llm_analysis.json - LLM analysis results")
    print("\n")


if __name__ == "__main__":
    test_llm_analyzer()
